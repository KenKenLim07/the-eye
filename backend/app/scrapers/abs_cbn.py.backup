import time
import logging
import random
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass
from playwright.sync_api import Browser
from bs4 import BeautifulSoup
from app.pipeline.normalize import build_article, NormalizedArticle
from app.scrapers.base import launch_browser
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingResult:
    articles: List[NormalizedArticle]
    errors: List[str]
    performance: Dict[str, float]
    metadata: Dict[str, Any]

class ABSCBNScraper:
    """Stealth scraper for ABS-CBN News with security hardening and RSS discovery."""

    BASE_URL = "https://news.abs-cbn.com"
    # UA pool to randomize fingerprint
    USER_AGENTS = [
        (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/125.0.0.0 Safari/537.36"
        ),
        (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        ),
        (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_5) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15"
        ),
    ]

    MIN_DELAY = 5.0
    MAX_DELAY = 12.0
    MAX_RETRIES = 3

    RSS_CANDIDATES = [
        "/rss",            # common
        "/rss.xml",        # alternative
        "/news/rss",       # section
        "/latest/rss",     # latest
    ]

    SELECTORS = {
        "title": [
            "h1.article__title",
            "h1.entry-title",
            "h1",
            ".article-title",
            "title"
        ],
        "content": [
            ".article-content p",
            ".article__content p",
            ".story-content p",
            ".post-content p",
            "article p",
            "p"
        ],
        "published_date": [
            "time[datetime]",
            ".article__date time",
            ".date",
            "time"
        ]
    }

    GOOGLE_NEWS_RSS = "https://news.google.com/rss/search?q=site:news.abs-cbn.com&hl=en-PH&gl=PH&ceid=PH:en"

    def __init__(self):
        self.session_start = time.time()

    def _human_delay(self):
        delay = random.uniform(self.MIN_DELAY, self.MAX_DELAY)
        logger.info(f"ABS-CBN: waiting {delay:.1f}s before next request (stealth)")
        time.sleep(delay)

    def _validate_url(self, url: str) -> bool:
        if not url:
            return False
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        if not parsed.netloc.endswith("abs-cbn.com"):
            logger.warning(f"ABS-CBN: blocked external URL {url}")
            return False
        return True

    def _sanitize_text(self, text: str) -> str:
        if not text:
            return ""
        text = text.replace('<script>', '').replace('</script>', '')
        text = text.replace('javascript:', '').replace('data:', '')
        text = text.replace('<', '&lt;').replace('>', '&gt;')
        return text.strip()

    def _extract_with_fallbacks(self, soup: BeautifulSoup, selectors: List[str]) -> Optional[str]:
        for sel in selectors:
            try:
                el = soup.select_one(sel)
                if el:
                    value = el.get_text().strip()
                    if value:
                        return self._sanitize_text(value)
            except Exception as e:
                logger.debug(f"ABS-CBN selector {sel} failed: {e}")
                continue
        return None

    def _discover_links_from_homepage(self, max_links: int = 12) -> List[str]:
        links: List[str] = []
        section_paths = ["/", "/latest", "/news", "/business", "/sports", "/entertainment"]
        with httpx.Client(follow_redirects=True, timeout=15.0, headers=self._http_headers()) as client:
            for path in section_paths:
                try:
                    url = urljoin(self.BASE_URL, path)
                    r = client.get(url)
                    if r.status_code >= 400:
                        continue
                    soup = BeautifulSoup(r.text, "html.parser")
                    for a in soup.select("a[href]"):
                        href = (a.get("href") or "").strip()
                        if not href:
                            continue
                        full = urljoin(self.BASE_URL, href)
                        if self._validate_url(full):
                            links.append(full)
                    if len(links) >= max_links:
                        break
                except Exception:
                    continue
        # dedupe
        seen = set()
        deduped: List[str] = []
        for l in links:
            if l not in seen:
                seen.add(l)
                deduped.append(l)
            if len(deduped) >= max_links:
                break
        return deduped

    def _discover_links_from_google_news(self, max_links: int = 20) -> List[str]:
        links: List[str] = []
        resolved_count = 0
        try:
            with httpx.Client(follow_redirects=True, timeout=15.0, headers=self._http_headers()) as client:
                r = client.get(self.GOOGLE_NEWS_RSS)
                if r.status_code >= 400:
                    logger.warning(f"ABS-CBN: Google News RSS returned {r.status_code}")
                    return []
                soup = BeautifulSoup(r.text, "xml")
                items = soup.select("item")
                logger.info(f"ABS-CBN: Google News items={len(items)}")
                for node in items:
                    if len(links) >= max_links:
                        break
                    resolved: Optional[str] = None
                    # 1) content:encoded (namespaced)
                    try:
                        enc = node.select_one("content\\:encoded")
                        if enc and enc.text:
                            enc_html = BeautifulSoup(enc.text, "html.parser")
                            # Prefer ABS-CBN link
                            for a in enc_html.find_all("a", href=True):
                                href = a["href"].strip()
                                if href.startswith("http") and "news.abs-cbn.com" in href:
                                    resolved = href
                                    break
                            if not resolved:
                                for a in enc_html.find_all("a", href=True):
                                    href = a["href"].strip()
                                    if not href.startswith("http"):
                                        continue
                                    if "news.google.com" in href or ".google." in href:
                                        continue
                                    resolved = href
                                    break
                    except Exception:
                        pass
                    # 2) <link> element: try url= param
                    if not resolved:
                        try:
                            ln = node.find("link")
                            if ln and ln.get_text():
                                gn = ln.get_text().strip()
                                if gn:
                                    from urllib.parse import urlparse, parse_qs
                                    p = urlparse(gn)
                                    direct = (parse_qs(p.query).get("url") or [None])[0]
                                    if direct:
                                        resolved = direct
                                    else:
                                        # 3) Request GN link and resolve/parse canonical
                                        try:
                                            resp = client.get(gn)
                                            if resp.status_code < 400:
                                                final_url = str(resp.url)
                                                if "news.abs-cbn.com" in final_url:
                                                    resolved = final_url
                                                else:
                                                    hsoup = BeautifulSoup(resp.text, "html.parser")
                                                    # canonical / og:url
                                                    link_canon = hsoup.find("link", rel=lambda v: v and "canonical" in v)
                                                    if link_canon and link_canon.get("href"):
                                                        href = link_canon.get("href").strip()
                                                        if href.startswith("http") and "google" not in href:
                                                            resolved = href
                                                    if not resolved:
                                                        meta_og = hsoup.find("meta", property="og:url")
                                                        if meta_og and meta_og.get("content"):
                                                            href = meta_og.get("content").strip()
                                                            if href.startswith("http") and "google" not in href:
                                                                resolved = href
                                                    if not resolved:
                                                        for a in hsoup.find_all("a", href=True):
                                                            href = a["href"].strip()
                                                            if not href.startswith("http"):
                                                                continue
                                                            if "news.abs-cbn.com" in href:
                                                                resolved = href
                                                                break
                                                    if not resolved:
                                                        for a in hsoup.find_all("a", href=True):
                                                            href = a["href"].strip()
                                                            if not href.startswith("http"):
                                                                continue
                                                            if "news.google.com" in href or ".google." in href:
                                                                continue
                                                            resolved = href
                                                            break
                                        except Exception:
                                            pass
                        except Exception:
                            pass
                    if resolved and self._validate_url(resolved):
                        links.append(resolved)
                        resolved_count += 1
                    elif resolved and "news.abs-cbn.com" in resolved:
                        links.append(resolved)
                        resolved_count += 1
                logger.info(f"ABS-CBN: Google News resolved {resolved_count} publisher URLs; returning {len(links)}")
        except Exception as e:
            logger.warning(f"ABS-CBN: Google News parse error: {e}")
            return []
        # dedupe
        seen = set()
        res: List[str] = []
        for l in links:
            if l not in seen:
                seen.add(l)
                res.append(l)
        return res

    def _discover_links_via_playwright(self, max_links: int = 20) -> List[str]:
        links: List[str] = []
        section_paths = ["/latest", "/news", "/business", "/sports", "/entertainment"]
        try:
            with launch_browser() as browser:
                context = self._new_context(browser)
                page = context.new_page()
                self._set_common_headers(page)
                self._apply_stealth(page)
                page.set_default_timeout(30000)
                page.set_default_navigation_timeout(30000)
                for path in section_paths:
                    if len(links) >= max_links:
                        break
                    url = urljoin(self.BASE_URL, path)
                    try:
                        resp = page.goto(url, wait_until='domcontentloaded')
                        if not resp or resp.status >= 400:
                            continue
                        try:
                            page.wait_for_selector('a', timeout=5000)
                        except:
                            pass
                        # gentle scroll to trigger lazy content
                        try:
                            for _ in range(3):
                                page.evaluate("window.scrollBy(0, document.body.scrollHeight/3)")
                                time.sleep(0.6)
                        except Exception:
                            pass
                        html = page.content()
                        soup = BeautifulSoup(html, 'html.parser')
                        for a in soup.select('a[href]'):
                            href = (a.get('href') or '').strip()
                            if not href:
                                continue
                            full = urljoin(self.BASE_URL, href)
                            if self._validate_url(full):
                                links.append(full)
                                if len(links) >= max_links:
                                    break
                    except Exception:
                        continue
                context.close()
        except Exception:
            return []
        # dedupe
        seen = set()
        deduped: List[str] = []
        for l in links:
            if l not in seen:
                seen.add(l)
                deduped.append(l)
            if len(deduped) >= max_links:
                break
        return deduped

    def _discover_links_from_google_news_playwright(self, max_links: int = 15) -> List[str]:
        links: List[str] = []
        try:
            # Query Google News HTML directly via Playwright search UI
            from urllib.parse import quote_plus
            query = f"https://www.google.com/search?tbm=nws&hl=en-PH&q={quote_plus('site:news.abs-cbn.com')}"
            with launch_browser() as browser:
                context = self._new_context(browser)
                page = context.new_page()
                self._set_common_headers(page)
                self._apply_stealth(page)
                page.set_default_timeout(25000)
                page.set_default_navigation_timeout(25000)
                try:
                    resp = page.goto(query, wait_until='domcontentloaded')
                    if resp and resp.status < 400:
                        try:
                            page.wait_for_selector('a', timeout=5000)
                        except Exception:
                            pass
                        # Scroll a bit to load more results
                        try:
                            for _ in range(2):
                                page.evaluate("window.scrollBy(0, document.body.scrollHeight/2)")
                                time.sleep(0.5)
                        except Exception:
                            pass
                        anchors = page.locator('a')
                        count = min(anchors.count(), 200)
                        for i in range(count):
                            if len(links) >= max_links:
                                break
                            try:
                                href = anchors.nth(i).get_attribute('href')
                                if not href or not href.startswith('http'):
                                    continue
                                if 'google' in href:
                                    continue
                                if 'news.abs-cbn.com' in href:
                                    links.append(href)
                            except Exception:
                                continue
                except Exception:
                    pass
                context.close()
        except Exception:
            return []
        # dedupe
        seen = set()
        out: List[str] = []
        for l in links:
            if l not in seen:
                seen.add(l)
                out.append(l)
        return out

    def _resolve_google_news_url(self, gn_url: str) -> Optional[str]:
        try:
            with httpx.Client(follow_redirects=True, timeout=15.0, headers=self._http_headers()) as client:
                from urllib.parse import urlparse, parse_qs
                p = urlparse(gn_url)
                direct = (parse_qs(p.query).get("url") or [None])[0]
                if direct and direct.startswith("http"):
                    return direct
                resp = client.get(gn_url)
                if resp.status_code >= 400:
                    return None
                final_url = str(resp.url)
                if final_url.startswith("http") and "news.abs-cbn.com" in final_url:
                    return final_url
                hsoup = BeautifulSoup(resp.text, "html.parser")
                link_canon = hsoup.find("link", rel=lambda v: v and "canonical" in v)
                if link_canon and link_canon.get("href"):
                    href = link_canon.get("href").strip()
                    if href.startswith("http") and "google" not in href:
                        return href
                meta_og = hsoup.find("meta", property="og:url")
                if meta_og and meta_og.get("content"):
                    href = meta_og.get("content").strip()
                    if href.startswith("http") and "google" not in href:
                        return href
                for a in hsoup.find_all("a", href=True):
                    href = a["href"].strip()
                    if not href.startswith("http"):
                        continue
                    if "news.abs-cbn.com" in href:
                        return href
                return None
        except Exception:
            return None

    def _extract_content(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        # Remove obvious boilerplate containers
        try:
            for sel in [
                "nav", "footer", "aside", ".share", ".social", ".related", ".recommend",
                ".tags", ".breadcrumbs", ".author", ".byline", ".comments", "#comments",
                ".ad", "[class*='advert']", "[id*='advert']", "script", "style", "template", "noscript",
            ]:
                for el in soup.select(sel):
                    el.decompose()
        except Exception:
            pass
        parts: List[str] = []
        for sel in self.SELECTORS["content"]:
            try:
                for el in soup.select(sel):
                    text = el.get_text(" ", strip=True)
                    if not text:
                        continue
                    if "{{" in text and "}}" in text:
                        continue
                    up = text.upper()
                    if up.startswith("ADVERTISEMENT"):
                        continue
                    if len(text) < 25 and not (text.endswith(".") or text.endswith("!") or text.endswith("?")):
                        continue
                    parts.append(self._sanitize_text(text))
                    if len(parts) >= 40:
                        break
                if parts:
                    break
            except Exception as e:
                logger.debug(f"ABS-CBN content selector {sel} failed: {e}")
                continue
        if not parts:
            for p in soup.find_all('p'):
                t = (p.get_text(" ", strip=True) or "").strip()
                if not t:
                    continue
                if "{{" in t and "}}" in t:
                    continue
                if t.upper().startswith("ADVERTISEMENT"):
                    continue
                if len(t) < 25 and not (t.endswith(".") or t.endswith("!") or t.endswith("?")):
                    continue
                parts.append(self._sanitize_text(t))
                if len(parts) >= 40:
                    break
        if parts:
            combined = "\n\n".join(parts)
            return combined[:20000]
        logger.warning(f"ABS-CBN: no content extracted for {url}")
        return None

    def _random_user_agent(self) -> str:
        return random.choice(self.USER_AGENTS)

    def _new_context(self, browser: Browser):
        # Stealth-ish fingerprint: PH locale/timezone, desktop viewport
        try:
            ctx = browser.new_context(
                user_agent=self._random_user_agent(),
                locale='en-PH',
                timezone_id='Asia/Manila',
                viewport={"width": 1366, "height": 768},
                java_script_enabled=True,
            )
        except TypeError:
            # Fallback if timezone_id unsupported in environment
            ctx = browser.new_context(
                user_agent=self._random_user_agent(),
                locale='en-PH',
                viewport={"width": 1366, "height": 768},
                java_script_enabled=True,
            )
        return ctx

    def _set_common_headers(self, page):
        page.set_extra_http_headers({
            'User-Agent': self._random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-PH,en;q=0.9',
            'Connection': 'keep-alive',
            'Referer': self.BASE_URL + '/',
            'Upgrade-Insecure-Requests': '1',
        })

    def _apply_stealth(self, page):
        try:
            page.add_init_script("""
                // navigator.webdriver false
                Object.defineProperty(navigator, 'webdriver', {get: () => false});
                // languages
                Object.defineProperty(navigator, 'languages', {get: () => ['en-PH', 'en']});
                // plugins
                Object.defineProperty(navigator, 'plugins', {get: () => [1,2,3]});
            """)
        except Exception:
            pass

    def _http_headers(self) -> Dict[str, str]:
        return {
            'User-Agent': self._random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-PH,en;q=0.9',
            'Connection': 'keep-alive',
            'Referer': self.BASE_URL + '/',
            'Upgrade-Insecure-Requests': '1',
        }

    def _fetch_rss_links(self, max_links: int = 6) -> List[str]:
        links: List[str] = []
        with httpx.Client(follow_redirects=True, timeout=15.0, headers=self._http_headers()) as client:
            for path in self.RSS_CANDIDATES:
                url = urljoin(self.BASE_URL, path)
                try:
                    resp = client.get(url)
                    if resp.status_code >= 400 or 'xml' not in resp.headers.get('content-type', ''):
                        logger.warning(f"ABS-CBN RSS: {url} returned {resp.status_code}")
                        continue
                    soup = BeautifulSoup(resp.text, 'xml')
                    for item in soup.select('item > link'):
                        href = (item.get_text() or '').strip()
                        if href and self._validate_url(href):
                            links.append(href)
                    if links:
                        break
                except Exception as e:
                    logger.warning(f"ABS-CBN RSS fetch error {url}: {e}")
                    continue
        deduped = []
        seen = set()
        for l in links:
            if l not in seen:
                deduped.append(l)
                seen.add(l)
            if len(deduped) >= max_links:
                break
        logger.info(f"ABS-CBN RSS: got {len(deduped)} links")
        return deduped

    def _to_amp(self, url: str) -> List[str]:
        candidates = []
        if url.endswith('/'):
            candidates.append(url + 'amp')
        else:
            candidates.append(url.rstrip('/') + '/amp')
        if '?amp' not in url:
            candidates.append(url + ('&amp' if '?' in url else '?amp'))
        candidates.append(url)
        return candidates

    def _fetch_article_http(self, url: str) -> Optional[BeautifulSoup]:
        with httpx.Client(follow_redirects=True, timeout=20.0, headers=self._http_headers()) as client:
            for candidate in self._to_amp(url):
                try:
                    r = client.get(candidate)
                    if r.status_code < 400 and r.text:
                        return BeautifulSoup(r.text, 'html.parser')
                    logger.debug(f"ABS-CBN HTTP {r.status_code} for {candidate}")
                except Exception as e:
                    logger.debug(f"ABS-CBN HTTP error for {candidate}: {e}")
                self._human_delay()
        return None

    def _scrape_article_via_playwright(self, url: str, browser: Browser) -> Optional[BeautifulSoup]:
        try:
            context = self._new_context(browser)
            page = context.new_page()
            self._set_common_headers(page)
            page.set_default_timeout(30000)
            page.set_default_navigation_timeout(30000)
            resp = page.goto(url, wait_until='domcontentloaded')
            if not resp or resp.status >= 400:
                context.close()
                return None
            try:
                page.wait_for_selector('h1', timeout=8000)
            except:
                pass
            soup = BeautifulSoup(page.content(), 'html.parser')
            context.close()
            return soup
        except Exception:
            return None

    def _build_article_from_soup(self, soup: BeautifulSoup, url: str) -> Optional[NormalizedArticle]:
        title = self._extract_with_fallbacks(soup, self.SELECTORS["title"]) or ""
        if not title:
            return None
        content = self._extract_content(soup, url)
        published = self._extract_with_fallbacks(soup, self.SELECTORS["published_date"]) or None
        return build_article(
            source="ABS-CBN News",
            category="General",
            title=title,
            url=url,
            content=content,
            published_at=published
        )

    def scrape_latest(self, max_articles: int = 3) -> ScrapingResult:
        start = time.time()
        articles: List[NormalizedArticle] = []
        errors: List[str] = []
        diag: Dict[str, Any] = {"playwright_links": 0, "homepage_links": 0, "gnews_links": 0, "rss_links": 0, "candidates": 0}
        diag_samples: Dict[str, List[str]] = {"playwright": [], "homepage": [], "gnews": [], "rss": [], "candidates": []}
        try:
            discovered: List[str] = []
            # 1) Playwright discovery (sections, stealth)
            if len(discovered) < max_articles * 2:
                p_links = self._discover_links_via_playwright(max_links=max_articles * 10)
                discovered.extend(p_links)
                diag["playwright_links"] = len(p_links)
                diag_samples["playwright"] = p_links[:5]
            # 2) Google News (HTTP parser)
            if len(discovered) < max_articles * 2:
                gnews = self._discover_links_from_google_news(max_links=max_articles * 10)
                discovered.extend(gnews)
                diag["gnews_links"] = len(gnews)
                diag_samples["gnews"] = gnews[:5]
            # 3) Google News Playwright (HTML results, stealth)
            if len(discovered) < max_articles * 2:
                gnews_playwright = self._discover_links_from_google_news_playwright(max_links=max_articles * 10)
                discovered.extend(gnews_playwright)
                diag["gnews_playwright_links"] = len(gnews_playwright)
                diag_samples["gnews_playwright"] = gnews_playwright[:5]
            # 4) (De-emphasize) RSS is often 403; skip unless nothing found
            if len(discovered) == 0:
                rss = self._fetch_rss_links(max_links=max_articles * 2)
                discovered.extend(rss)
                diag["rss_links"] = len(rss)
                diag_samples["rss"] = rss[:5]
            # Dedup and validate
            seen = set()
            candidates: List[str] = []
            for u in discovered:
                if u not in seen and self._validate_url(u):
                    seen.add(u)
                    candidates.append(u)
                elif u in seen and not self._validate_url(u):
                    # If a URL was already seen but failed validation, try resolving it
                    resolved = self._resolve_google_news_url(u)
                    if resolved and self._validate_url(resolved):
                        seen.add(resolved)
                        candidates.append(resolved)
                    else:
                        logger.warning(f"ABS-CBN: failed to validate {u} or resolve to a valid URL. Skipping.")
                elif u not in seen and not self._validate_url(u):
                    logger.warning(f"ABS-CBN: failed to validate {u}. Skipping.")
            diag["candidates"] = len(candidates)
            diag_samples["candidates"] = candidates[:5]
            # Fetch articles
            with launch_browser() as browser:
                for i, url in enumerate(candidates[:max_articles]):
                    soup = self._fetch_article_http(url)
                    if not soup:
                        logger.info("ABS-CBN: falling back to Playwright for article")
                        soup = self._scrape_article_via_playwright(url, browser)
                    if soup:
                        art = self._build_article_from_soup(soup, url)
                        if art:
                            articles.append(art)
                            logger.info(f"ABS-CBN: scraped {art.title}")
                        else:
                            errors.append(f"failed to extract {url}")
                    else:
                        errors.append(f"failed to load {url}")
                    if i < min(len(candidates), max_articles) - 1:
                        self._human_delay()
        except Exception as e:
            msg = f"ABS-CBN: critical error {e}"
            logger.error(msg)
            errors.append(str(e))
        total = time.time() - start
        performance = {
            "total_time": total,
            "articles_per_second": len(articles) / total if total > 0 else 0,
            "success_rate": len(articles) / (len(articles) + len(errors)) if (len(articles) + len(errors)) > 0 else 0
        }
        metadata = {
            "source": "ABS-CBN News",
            "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_articles_found": len(articles),
            "total_errors": len(errors),
            "discovery": diag,
            "samples": diag_samples
        }
        logger.info(f"ABS-CBN: completed {len(articles)} articles, {len(errors)} errors in {total:.2f}s")
        return ScrapingResult(articles=articles, errors=errors, performance=performance, metadata=metadata)


def scrape_abs_cbn_latest() -> List[NormalizedArticle]:
    scraper = ABSCBNScraper()
    result = scraper.scrape_latest(max_articles=3)
    if result.errors:
        logger.warning(f"ABS-CBN: errors {result.errors}")
    return result.articles 